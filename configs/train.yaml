# @package _global_

defaults:
  - _self_
  - paths: default.yaml
  - hydra: default.yaml
  - dmt/datamodule: ???
  - dmt/model: ???
  - dmt/trainer: default.yaml
  - optional local: default.yaml

  # debugging config (enable through command line, e.g. `python train.py debug=default)
  - debug: null

# seed for random number generators in pytorch, numpy and python.random
seed: 42

# Output directory is generated dynamically on each run
# following the pattern ${paths.log_dir}/${task_name}/runs/${run_name}
task_name: train
run_name: ${dmt.datamodule.augmentation}/${now:%Y-%m-%d}_${now:%H-%M-%S}

# set False to skip model training
train: True

# evaluate on test set, using best model weights achieved during training
# lightning chooses best weights based on the metric specified in checkpoint callback
test: True

# simply provide checkpoint path to resume training
ckpt_path: null

